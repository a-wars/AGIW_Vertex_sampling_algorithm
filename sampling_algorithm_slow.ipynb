{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling algorithm\n",
    "---\n",
    "Implementation of Vertex's sampling algorithm described in [_Web-scale information extraction with vertex_](https://ieeexplore.ieee.org/abstract/document/5767842)\n",
    "\n",
    "## Outline\n",
    "---\n",
    "1. [Retrieve XPaths from a page](#1.-retrieve-xpaths-from-a-page)\n",
    "2. [Compute XPath weight]()\n",
    "3. [Sampling algorithm]()\n",
    "\n",
    "## 1. Retrieve XPaths from a page\n",
    "As reported in the paper:\n",
    "\n",
    "> One simple way to achieve this is to treat each page as a set of XPaths contained in it, and then greedily select pages that cover the most number of uncovered XPaths. \n",
    "\n",
    "However, the paper does not specify which Xpaths are extracted from a page. Therefore we decided to extract XPaths which retrieves textual leaf nodes.\n",
    "\n",
    "To do so we used the [lxml]() library to select all leaf textual nodes in a page. Then, using the same library we obtained the respective XPath of each leaf node previously selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from lxml import html\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``get_all_xpath``\n",
    "Given a page HTML source code returns a dict { _xpath_ : _value_ }, where _xpath_ is an xpath and _value_ is the string retrieved from the xpath on _src_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_xpath(html_src):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # select nodes whose children include text nodes\n",
    "    XPATH_SELECTOR = \"//*[child::text()]\" \n",
    "        \n",
    "    root = html.fromstring(html_src)\n",
    "    \n",
    "    tree = root.getroottree()\n",
    "    \n",
    "    # leaf_nodes is not properly a list of all leaf nodes. \n",
    "    # It contains nodes which are parent of text elements in the DOM\n",
    "    leaf_nodes = root.xpath(XPATH_SELECTOR)\n",
    "    \n",
    "    xpath_value_dict = {}\n",
    "    \n",
    "    # extract xpath from previously selected nodes and filter out \"noisy\" nodes\n",
    "    for leaf in leaf_nodes:\n",
    "        \n",
    "        xpath = tree.getpath(leaf) + \"/text()\"\n",
    "        \n",
    "        # Filtering out xpaths which extract javascript code or css stylesheet\n",
    "        if  \"/script\" not in xpath and \"/noscript\" not in xpath and \"/style\" not in xpath:\n",
    "                        \n",
    "            selected_values = root.xpath(xpath)\n",
    "            selected_string = ''.join(selected_values).strip()\n",
    "            \n",
    "            # Filtering out xpaths which extract empty strings\n",
    "            if selected_string:\n",
    "                xpath_value_dict.update({xpath: selected_string})\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    t = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print('INFO\\tElapsed time in get_all_xpath function:', t)\n",
    "    \n",
    "    return xpath_value_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute XPaths weights\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_data_structures\n",
    "Return necessary data structures for computing xpaths weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_structures(url_to_html_map):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    url_to_xpaths = {}\n",
    "    xpath_to_value_list = defaultdict(list)\n",
    "    \n",
    "    for url in list(url_to_html_map):\n",
    "        \n",
    "        page = url_to_html_map[url]\n",
    "        xpath_to_single_value = get_all_xpath(page)\n",
    "        xpath_list = list(xpath_to_single_value)\n",
    "        url_to_xpaths[url] = xpath_list\n",
    "                \n",
    "        for xpath in xpath_to_single_value:\n",
    "            value = xpath_to_single_value[xpath]\n",
    "            xpath_to_value_list[xpath].append(value)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    t = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print('INFO\\tElapsed time in get_data_structures:', t)\n",
    "    \n",
    "    return (url_to_xpaths, xpath_to_value_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute frequency\n",
    "Given a list of values extracted from a xpath _Xi_ returns the frequency of _Xi_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency(values_list):\n",
    "    return len(values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute informativeness\n",
    "Given cluster size and a list of values extracted from a xpath _Xi_ returns the informativeness of _Xi_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_informativeness(M, values_list):\n",
    "\n",
    "    values_set = set(values_list)\n",
    "    Ti = len(values_set)\n",
    "    \n",
    "    sum_F_Xi = compute_frequency(values_list)\n",
    "\n",
    "    return 1 - sum_F_Xi/(M*Ti)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xpath weight\n",
    "Given a list of values extracted from a xpath _Xi_ returns the weight of _Xi_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xpath_weight(cluster_size, list_of_values):\n",
    "    return compute_frequency(list_of_values)*compute_informativeness(cluster_size, list_of_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xpath_to_weight\n",
    "Arguments:\n",
    "- **xpath_to_values_map**: dictionary where keys are xpath and values are values retrieved from the xpath\n",
    "- **cluster_size**\n",
    "\n",
    "Returns a dictionary where keys are xpaths and values are their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xpath_to_weight(xpath_to_values_map, cluster_size):\n",
    "    \n",
    "    result = {}\n",
    "    for xpath in xpath_to_values_map:\n",
    "        list_of_values = xpath_to_values_map[xpath]\n",
    "        weight = xpath_weight(cluster_size, list_of_values)\n",
    "        result.update({xpath: weight})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### page_weight\n",
    "Arguments:\n",
    "- **list of xpath**: list of xpath of a given page\n",
    "- **xpath_to_weight_map**: dictionary where keys are xpath and values are their weights\n",
    "- **cluster_size**\n",
    "- **intersection** (optional): if None nothing happens. Otherwise only xpath in **list of xpath** $\\cap$ **intersection** will be considered in computing weight\n",
    "\n",
    "Returns page's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_weight(list_of_xpath, xpath_to_weight_map, cluster_size, intersection = None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    weight = 0\n",
    "    \n",
    "    if intersection is None:\n",
    "        intersection = list_of_xpath\n",
    "        \n",
    "    for xpath in list_of_xpath:\n",
    "        if xpath in intersection:\n",
    "            weight_of_xpath = xpath_to_weight_map[xpath]\n",
    "            weight += weight_of_xpath\n",
    "            \n",
    "    elapsed_time = time.time() - start_time\n",
    "    t = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print('INFO\\tElapsed time in page_weight function:', t)\n",
    "    \n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max weight page\n",
    "Arguments:\n",
    "- **url_to_xpaths_map**: dictionary where keys are urls and values are xpaths extracted from urls\n",
    "- **xpath_to_weight_map**: dictionary where keys are xpath and values are their weights\n",
    "- **cluster_size**\n",
    "- **intersection** (optional): if None nothing happens. Otherwise only xpath in **list of xpath** $\\cap$ **intersection** will be considered in computing weight\n",
    "\n",
    "Output: \n",
    "- the URL of the page with the highest weight value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_weight_page(url_to_xpaths_map, xpath_to_weight_map, cluster_size, intersection = None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    max_weight = 0\n",
    "    max_weight_page = None\n",
    "    \n",
    "    for url in url_to_xpaths_map:\n",
    "        \n",
    "        xpaths = url_to_xpaths_map[url]\n",
    "        weight = page_weight(xpaths, xpath_to_weight_map, cluster_size, intersection)\n",
    "        \n",
    "        if weight > max_weight:\n",
    "            max_weight = weight\n",
    "            max_weight_page = url\n",
    "            \n",
    "    print(\"INFO\\tMax weight url is  {}\".format(max_weight_page))\n",
    "    print(\"INFO\\tMax weight url is  {}\".format(max_weight))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    t = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print('INFO\\tElapsed time in max_weight_page function:', t)\n",
    "    \n",
    "    return max_weight_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coverage\n",
    "Returns cluster's page coverage. TODO: add more explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(X, sample_pages_urls, cluster_pages_urls, url_to_xpaths_map, xpath_to_weight_map):\n",
    "    covered = 0\n",
    "    cluster_size = len(cluster_pages_urls)\n",
    "    for url in cluster_pages_urls:\n",
    "        if url not in sample_pages_urls:\n",
    "            xpaths = url_to_xpaths_map[url]\n",
    "            weight = page_weight(xpaths, xpath_to_weight_map, cluster_size, X)\n",
    "            if weight == 0:\n",
    "                covered = covered + 1\n",
    "    \n",
    "    return (covered + len(sample_pages_urls))/cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another metric to evaluate sample coverage\n",
    "def coverage2(samplePagesUrl,urlToXpathsMap):\n",
    "    #list that can contain repetitions\n",
    "    xpathList=urlToXpathMap.values()\n",
    "    allXpathSet=set(xpathList)\n",
    "    sampleXpathList=[]\n",
    "    for url in urlToXpathsMap:\n",
    "        xpaths=urlToXpathsMap[url]\n",
    "        sampleXpathList=sampleXpathList+xpaths\n",
    "    sampleXpathSet=set(sampleXpathList)\n",
    "    return (len(sampleXpathSet)/len(allXpathSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(url_to_html_map, k = 20):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cluster_size = len(url_to_html_map)\n",
    "    \n",
    "    cluster_pages_urls = list(url_to_html_map)\n",
    "    \n",
    "    url_to_xpaths_map, xpath_to_values_map = get_data_structures(url_to_html_map)\n",
    "    \n",
    "    xpath_to_weight_map = xpath_to_weight(xpath_to_values_map, cluster_size)\n",
    "    \n",
    "    X = list(xpath_to_values_map) #insert dictionary keys into a list\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    iteration_no = 1\n",
    "    \n",
    "    while X and len(result) < k:\n",
    "        print(\"-------------------\")\n",
    "        print(\"INFO\\tIteration {}\".format(iteration_no))\n",
    "        \n",
    "        max_weight_url = max_weight_page(url_to_xpaths_map, xpath_to_weight_map, cluster_size, X)\n",
    "        result.append(max_weight_url)\n",
    "        X = [xpath for xpath in X if xpath not in url_to_xpaths_map[max_weight_url]]\n",
    "        url_to_xpaths_map.pop(max_weight_url)\n",
    "        \n",
    "        coverage_value = coverage(X, result, cluster_pages_urls, url_to_xpaths_map, xpath_to_weight_map)\n",
    "        \n",
    "        print(\"INFO\\tCoverage is {}\".format(coverage_value))\n",
    "        \n",
    "        iteration_no = iteration_no +1\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    t = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    print('INFO\\tElapsed time in sampling function:', t)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'imdb_dataset.csv' does not exist: b'imdb_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a24710d2441e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imdb_dataset.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\agiw_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\agiw_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\agiw_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\agiw_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\agiw_env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'imdb_dataset.csv' does not exist: b'imdb_dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imdb_dataset.csv', nrows = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>http://www.imdb.com/name/nm4303490</td>\n",
       "      <td>b'&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url  \\\n",
       "count                                   10   \n",
       "unique                                  10   \n",
       "top     http://www.imdb.com/name/nm4303490   \n",
       "freq                                     1   \n",
       "\n",
       "                                                      src  \n",
       "count                                                  10  \n",
       "unique                                                 10  \n",
       "top     b'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(df):\n",
    "    result = {}\n",
    "    for index, row in df.iterrows():\n",
    "        key = row['url']\n",
    "        value = row['src']\n",
    "        result.update({key: value})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_all_xpath function: 00:00:00\n",
      "INFO\tElapsed time in get_data_structures: 00:00:00\n",
      "-------------------\n",
      "INFO\tIteration 1\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm2246776\n",
      "INFO\tMax weight url is  603.679999999998\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 0.1\n",
      "-------------------\n",
      "INFO\tIteration 2\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm4036436\n",
      "INFO\tMax weight url is  181.9916666666666\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 0.2\n",
      "-------------------\n",
      "INFO\tIteration 3\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm3105206\n",
      "INFO\tMax weight url is  45.79999999999997\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 0.3\n",
      "-------------------\n",
      "INFO\tIteration 4\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm1573499\n",
      "INFO\tMax weight url is  28.274999999999984\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 0.7\n",
      "-------------------\n",
      "INFO\tIteration 5\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm2091436\n",
      "INFO\tMax weight url is  9.900000000000002\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 0.8\n",
      "-------------------\n",
      "INFO\tIteration 6\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm2391180\n",
      "INFO\tMax weight url is  8.100000000000001\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 0.9\n",
      "-------------------\n",
      "INFO\tIteration 7\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tMax weight url is  http://www.imdb.com/name/nm1244056\n",
      "INFO\tMax weight url is  3.6\n",
      "INFO\tElapsed time in max_weight_page function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tElapsed time in page_weight function: 00:00:00\n",
      "INFO\tCoverage is 1.0\n",
      "INFO\tElapsed time in sampling function: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "cluster = create_dict(df)\n",
    "sample_pages = sampling(cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
